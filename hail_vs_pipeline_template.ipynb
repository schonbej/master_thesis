{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template for GWAS pipeline with logistic regression and VariantSpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- __What does this pipeline do?__\n",
    "    - Load data from PLINK or Hail format\n",
    "    - Quality Control\n",
    "    - Principal Component Analysis\n",
    "    - GWAS: logistic regression (QQ, Manhattan plot)\n",
    "    - VariantSpark: random forest (Manhattan plot)\n",
    "    - Scatter plot comparing SNP scores from VariantSpark and logistic regression\n",
    "    - (if file is provided: overlap with GWAS catalog)\n",
    "    - Polygenic Risk Score calculation\n",
    "    - Prediction accuracy evaluation with random forest on top n most significant SNPs from GWAS and VS\n",
    "- __What is not in here?__\n",
    "    - Annotation with Hail database functionality since only possible with Hail version that is incompatible with VS (and on GCP)\n",
    "    - Burden testing (reliant on annotation)\n",
    "- __How do I execute it?__\n",
    "    - Set paths and necessary parameters in the section <a href='#param'>parameters</a>\n",
    "    - Adjust parameters in each cell of the <a href='#pipeline'>pipeline</a> if needed (i.e. for QC)\n",
    "    - To see implementation go to <a href='#functions'>functions</a>\n",
    "\n",
    "__Note__: Only execute the <a href='#loading'>loading</a> of Hail, VariantSpark and other packages once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "The GWAS pipeline is based on \n",
    "[A guide to genome‐wide association analysis and post‐analytic interrogation](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.6605)\n",
    "and \n",
    "[A tutorial on conducting genome‐wide association studies: Quality control and statistical analysis](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6001694/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='param'></a>\n",
    "\n",
    "# Parameters\n",
    "Set paths and choose parameters for the pipeline here   \n",
    "Other parameters can be set in the <a href='#functions'>functions</a> section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading\n",
    "# set only_plink = True, if the data is only available in PLINK format. If False, the plink_path is ignored\n",
    "only_plink = False \n",
    "# adjust path to data in PLINK format\n",
    "plink_paths = {\n",
    "    'bed':'s3://tb-als/ukbio/ukb_efe_chr1_v1.bed',\n",
    "    'bim': 's3://tb-als/ukbio/ukb_fe_exm_chrall_v1.bim',\n",
    "    'fam': 's3://tb-als/ukbio/ukb27483_efe_chr1_v1_s49953.fam'\n",
    "}\n",
    "ref_genome = 'GRCh38'\n",
    "# path to data in Hail format (path to load from or save to)\n",
    "hl_path = \"s3://tb-als/ukbio/results/\"\n",
    "hl_name = \"asthma.mt\"\n",
    "\n",
    "\n",
    "## PCA\n",
    "# plot the first few PCs against each other and a scree plot\n",
    "PCA_visual = True\n",
    "\n",
    "## GWAS logistic regression and VS random forest\n",
    "# perform gwas catalog overlap or not\n",
    "gwas_catalog_overlap = False\n",
    "gwas_catalog_filepath = \"\"\n",
    "# the files produced here will also be saved into the hl_path folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loading'></a>\n",
    "\n",
    "# Loading Hail, VariantSpark and other packages\n",
    "Only execute this section once in a running notebook. Otherwise Hail fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import varspark.hail as vshl\n",
    "vshl.init(sc=sc, default_reference=ref_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hail.plot import show\n",
    "from pprint import pprint\n",
    "hl.plot.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import varspark.hail.plot as vshlplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install myvariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myvariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install treeinterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='functions'></a>\n",
    "\n",
    "# Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and converting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PLINK to hail matrix table\n",
    "# this needs lots of CPUs\n",
    "\n",
    "def plink_to_hl(plink_paths, hl_path):\n",
    "    print(\"Reading PLINK data from \" + plink_path + \". This might take some time.\")\n",
    "    start = time.time()\n",
    "    mt = hl.import_plink(bed=plink_paths['bed'], bim=plink_paths['bim'], fam=plink_paths['fam'],\n",
    "                         reference_genome=ref_genome)\n",
    "    end = time.time()\n",
    "    print(\"Loading as PLINK took \"+ str(round(end-start)) + \" seconds\")\n",
    "    # save as hail mt to hl_path\n",
    "    print(\"Writing hail matrix table to \" + hl_path + \".\")  \n",
    "    start = time.time()\n",
    "    mt.write(hl_path)\n",
    "    end = time.time()\n",
    "    print(\"Writing Hail matrix table took \"+ str(round(end-start)) + \" seconds\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To choose how many principal components should be used as covariates in logistic regression\n",
    "# plots a scree plot (eigenvalues) and PCs against each other to see how well they separate the data\n",
    "\n",
    "def choose_pcs(mt,k=5, annotated=False):\n",
    "    if not annotated:\n",
    "        eigenvalues, scores, loadings = hl.hwe_normalized_pca(mt.GT, k=k)\n",
    "        mt = mt.annotate_cols(scores = scores[mt.s])\n",
    "        pc_names = []\n",
    "        for i in range(len(eigenvalues)):\n",
    "            pc_names.append(\"PC\"+str(i+1))\n",
    "        #In multivariate statistics, a scree plot is a line plot of the eigenvalues of factors or principal components in an analysis.\n",
    "        #The scree plot is used to determine the number of factors to retain in an exploratory factor analysis (FA) or principal components to keep in a principal component analysis (PCA).\n",
    "\n",
    "        #A scree plot always displays the eigenvalues in a downward curve, ordering the eigenvalues from largest to smallest.\n",
    "        #According to the scree test, the \"elbow\" of the graph where the eigenvalues seem to level off is found and factors or components to the left of this point should be retained as significant\n",
    "\n",
    "        # scree plot\n",
    "        pc_names = []\n",
    "        for i in range(k):\n",
    "            pc_names.append(\"PC\"+str(i+1))\n",
    "        plt.bar(pc_names,eigenvalues)\n",
    "        plt.show()\n",
    "\n",
    "    # plot pcs 1-4 against each other\n",
    "    p = hl.plot.scatter(mt.scores.scores[0],\n",
    "                        mt.scores.scores[1],\n",
    "                        label=mt.is_case,\n",
    "                        title='PCA', xlabel='PC1', ylabel='PC2')\n",
    "    show(p)\n",
    "    p = hl.plot.scatter(mt.scores.scores[0],\n",
    "                        mt.scores.scores[2],\n",
    "                        label=mt.is_case,\n",
    "                        title='PCA', xlabel='PC1', ylabel='PC3')\n",
    "    show(p)\n",
    "    p = hl.plot.scatter(mt.scores.scores[0],\n",
    "                        mt.scores.scores[3],\n",
    "                        label=mt.is_case,\n",
    "                        title='PCA', xlabel='PC1', ylabel='PC4')\n",
    "    show(p)\n",
    "    p = hl.plot.scatter(mt.scores.scores[1],\n",
    "                        mt.scores.scores[2],\n",
    "                        label=mt.is_case,\n",
    "                        title='PCA', xlabel='PC2', ylabel='PC3')\n",
    "    show(p)\n",
    "    p = hl.plot.scatter(mt.scores.scores[1],\n",
    "                        mt.scores.scores[3],\n",
    "                        label=mt.is_case,\n",
    "                        title='PCA', xlabel='PC2', ylabel='PC4')\n",
    "    show(p)\n",
    "    p = hl.plot.scatter(mt.scores.scores[2],\n",
    "                        mt.scores.scores[3],\n",
    "                        label=mt.is_case,\n",
    "                        title='PCA', xlabel='PC3', ylabel='PC4')\n",
    "    show(p)\n",
    "    \n",
    "    return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty genotype calls with 0/0\n",
    "def replace_empty_gt(mt):\n",
    "    # or_else(a,b): if a is missing, return b\n",
    "    mt = mt.annotate_entries(GT=hl.or_else(mt.GT, hl.call(0, 0, phased=False)))\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call rate filter (variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out VARIANTS with low call rate \n",
    "# (thresh 1 needed for it to work with vs -> to get comparable results also do before logistic regression)\n",
    "def call_rate_filter_variants(mt, thresh=0.95):\n",
    "    print(\">>>>> call_rate_filter_variants\")\n",
    "    mt = hl.variant_qc(mt)\n",
    "    before_filter = mt.count_rows()\n",
    "    mt = mt.filter_rows(mt.variant_qc.call_rate >= thresh, keep=True)\n",
    "    print('After call rate filter (VARIANTS) with threshold '+str(thresh)+', '+str(mt.count_rows()) + '/'+ str(before_filter)+' variants remain.')\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minor allele frequency filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variants with a low minor allele frequency are filtered out\n",
    "\n",
    "def maf_filter(mt, thresh=0.01):\n",
    "    print(\">>>>> maf_filter with threshold \" + str(thresh))\n",
    "    mt = hl.variant_qc(mt)\n",
    "    before_filter = mt.count_rows()\n",
    "    mt = mt.filter_rows(hl.min(mt.variant_qc.AF) < thresh, keep=False)\n",
    "    print(\"After MAF filter with threshold \"+str(thresh)+\", \" + str(mt.count_rows()) + \"/\" + str(before_filter) + \" variants remain.\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linkage disequilibrium pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This removes variants that are in high LD to reduce redundance\n",
    "def ld_prune(mt, r2=0.2, window_size=500000):\n",
    "    print(\">>>>> ld_prune\")\n",
    "    mt = hl.variant_qc(mt)\n",
    "    before_filter = mt.count_rows()\n",
    "    #biallelic_mt = mt.filter_rows(hl.len(mt.alleles) == 2)\n",
    "    #if biallelic_mt.count() == mt.count():\n",
    "    start = time.time()\n",
    "    pruned_variant_table_r02 = hl.ld_prune(mt.GT, r2=r2, bp_window_size=window_size)\n",
    "    end = time.time()\n",
    "    print(\"Pruning took \" + str(round(end-start)) + \" seconds\")\n",
    "    mt = mt.filter_rows( hl.is_defined(pruned_variant_table_r02[mt.row_key]))\n",
    "    print(\"After LD pruning with r2=\"+str(r2)+\", \" + str(mt.count_rows()) + \"/\" + str(before_filter) + \" SNPs left\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardy Weinberg equilibrium filter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options to use the same filter for all samples (all_together) or use different thresholds for cases and controls (control_sep)\n",
    "def hwe_filter(mt, case_control=\"all_together\"):\n",
    "    print(\">>>>> hwe_filter\")\n",
    "    mt = hl.variant_qc(mt)\n",
    "    before_filter = mt.count_rows()\n",
    "    # changed control_sep to different thresholds of HWE filtering\n",
    "    if case_control == \"control_sep\":\n",
    "        case = mt.filter_cols(mt.is_case, keep=True)\n",
    "        ctrl = mt.filter_cols(~mt.is_case, keep=True)\n",
    "        ctrl = hl.variant_qc(ctrl)\n",
    "        ctrl = ctrl.filter_rows(ctrl.variant_qc.p_value_hwe < 1e-10, keep=False)\n",
    "        case = hl.variant_qc(case)\n",
    "        case = case.filter_rows(case.variant_qc.p_value_hwe < 1e-6, keep=False)\n",
    "        mt = case.union_cols(ctrl)\n",
    "    # all together applies thresh of 1e-6\n",
    "    elif case_control == \"all_together\":\n",
    "        mt = hl.variant_qc(mt)\n",
    "        mt = mt.filter_rows(mt.variant_qc.p_value_hwe < 0.000001, keep=False)\n",
    "    else:\n",
    "        print(\"WRONG OPTION. use 'all_together' or 'control_sep'\")\n",
    "    print(\"After HWE filter, \" + str(mt.count_rows()) + \"/\" + str(before_filter) + \" SNPs left\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autosome filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use autosomes (exclude X,Y, mitochondrial and other contigs)\n",
    "def autosomes(mt):\n",
    "    print(\">>>>> only use autosomes\")\n",
    "    mt = hl.variant_qc(mt)\n",
    "    before_filter = mt.count_rows()\n",
    "    # in_autosome_or_par() returns True if the locus is on an autosome or a pseudoautosomal region of chromosome X or Y\n",
    "    mt =  mt.filter_rows(mt.locus.in_autosome_or_par())\n",
    "    print(\"After autosome filter, \" + str(mt.count_rows()) + \"/\" + str(before_filter) + \" SNPs left\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single nucleotide filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variations involving multiple nucleotides are filtered out\n",
    "def single_np(mt):\n",
    "    print(\">>>>> only use single nucleotide variants (not multiple nucleotides)\")\n",
    "    mt = hl.variant_qc(mt)\n",
    "    before_filter = mt.count_rows()\n",
    "    # hl.is_snp('A', 'T')\n",
    "    #mt =  mt.filter_rows((mt.alleles[0],mt.alleles[1]).is_snp())\n",
    "    mt = hl.filter_alleles(mt, lambda allele, i: hl.is_snp(mt.alleles[0], allele))\n",
    "    # filter_alleles() does not update any fields other than locus and alleles. This means that row fields like allele count (AC) and entry fields like allele depth (AD) can become meaningless unless they are also updated. You can update them with annotate_rows() and annotate_entries().\n",
    "    mt = hl.variant_qc(mt)\n",
    "    print(\"After is_snp filter, \" + str(mt.count_rows()) + \"/\" + str(before_filter) + \" SNPs left\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call rate filter (samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_rate_filter_samples(mt, thresh=0.95):\n",
    "    print(\">>>>> call rate filter (samples) with threshold \" + str(thresh))\n",
    "    mt = hl.sample_qc(mt)\n",
    "    before_filter = mt.count_cols()\n",
    "    n_cases_before = mt.filter_cols(mt.is_case).count_cols()\n",
    "    mt = mt.filter_cols(mt.sample_qc.call_rate >= thresh)\n",
    "    n_cases_after = mt.filter_cols(mt.is_case).count_cols()\n",
    "    print('After call rate filter (SAMPLES) with threshold '+str(thresh)+', '+str(mt.count_cols()) + '/'+ str(before_filter)+' samples, including '+str(n_cases_after)+'/'+str(n_cases_before)+' cases, remain.')\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputed vs reported sex discrepancy filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove samples where reported sex differs from imputed sex\n",
    "def sex_discrepancy(mt):\n",
    "    print(\">>>>> check reported vs imputed sex\")\n",
    "    mt = hl.sample_qc(mt)\n",
    "    before = mt.count_cols()\n",
    "    n_cases_before = mt.filter_cols(mt.is_case).count_cols()\n",
    "    imputed_sex = hl.impute_sex(mt.GT)\n",
    "    mt = mt.filter_cols(imputed_sex[mt.s].is_female != mt.is_female, keep=False)\n",
    "    after = mt.count_cols()\n",
    "    n_cases_after = mt.filter_cols(mt.is_case).count_cols()\n",
    "    print(\"After filtering for discrepancies between imputed and reported sex \"+str(after) + \"/\" + str(before) + \" samples, including \"+str(n_cases_after)+\"/\"+str(n_cases_before)+\" cases, remain.\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heterozygosity filter\n",
    "From [Hail discussion board](https://discuss.hail.is/t/filtering-samples-with-extreme-heterozygosity-in-hail/1277/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (This takes a lot of time and sometimes breaks, consider running this separately to check if it affects sample count at all)\n",
    "# Samples with extreme heterozygosity are filtered out\n",
    "# removing individuals who deviate ±3 SD from the samples' heterozygosity rate mean.\n",
    "def het_filter(mt):\n",
    "    print(\">>>>> heterozygosity filter\")\n",
    "    mt = hl.sample_qc(mt)\n",
    "    before = mt.count_cols()\n",
    "    n_cases_before = mt.filter_cols(mt.is_case).count_cols()\n",
    "    mt=mt.annotate_cols(\n",
    "        heterozygosity=(mt.sample_qc.n_het/mt.sample_qc.n_called), \n",
    "        inbreeding = hl.agg.inbreeding(mt.GT, mt.variant_qc.AF[1]))\n",
    "    het_stats = mt.aggregate_entries(hl.agg.stats(mt.heterozygosity))\n",
    "    mt.filter_cols(mt.heterozygosity > het_stats.mean + 3*het_stats.stdev, keep=False)\n",
    "    mt.filter_cols(mt.heterozygosity < het_stats.mean - 3*het_stats.stdev, keep=False)\n",
    "    after = mt.count_cols()\n",
    "    n_cases_after = mt.filter_cols(mt.is_case).count_cols()\n",
    "    print(\"After heterozygosity filtering \"+str(after) + \"/\" + str(before) + \" samples, including \"+str(n_cases_after)+\"/\"+str(n_cases_before)+\" cases, remain.\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IBD pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for removal of controls over cases\n",
    "def tie_breaker(l, r):\n",
    "    return hl.cond(l.is_case & ~r.is_case, -1, hl.cond(~l.is_case & r.is_case, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes samples with identity by descent measure of >0.125 (third degree relatedness)\n",
    "def ibd_prune(mt, removal=\"max_ind_set\", set_min=0.125):\n",
    "    print(\">>>>> IBD pruning (this should happen AFTER LD pruning!)\")\n",
    "    mt = hl.sample_qc(mt)\n",
    "    before = mt.count_cols()\n",
    "    n_cases_before = mt.filter_cols(mt.is_case).count_cols()\n",
    "    # create ibd table of samples with ibd higher than 0.125 (third degree relatedness)\n",
    "    start = time.time()\n",
    "    ibd_matrix_min = hl.identity_by_descent(mt, min=set_min)\n",
    "    if removal==\"max_ind_set\":\n",
    "        # remove the maximal independent set of samples in the table\n",
    "        related_samples_to_remove = hl.maximal_independent_set(ibd_matrix_min.i, ibd_matrix_min.j, False)\n",
    "        mt = mt.filter_cols(hl.is_defined(related_samples_to_remove[mt.col_key]), keep=False)\n",
    "    elif removal==\"cases_over_controls\":\n",
    "        samples = mt.cols()\n",
    "        pairs_with_case = ibd_matrix_min.key_by(\n",
    "            i=hl.struct(id=ibd_matrix_min.i, is_case=samples[ibd_matrix_min.i].is_case),\n",
    "            j=hl.struct(id=ibd_matrix_min.j, is_case=samples[ibd_matrix_min.j].is_case))\n",
    "        related_samples_to_remove = hl.maximal_independent_set(\n",
    "            pairs_with_case.i, pairs_with_case.j, False, tie_breaker)\n",
    "        mt = mt.filter_cols(hl.is_defined(\n",
    "            related_samples_to_remove.key_by(\n",
    "            s = related_samples_to_remove.node.id.s)[mt.col_key]), keep=False)\n",
    "    else:\n",
    "         raise Exception(\"Wrong option for 'removal'. Use max_ind_set or cases_over_controls\")\n",
    "    end = time.time()\n",
    "    print(\"IBD pruning took \"+ str(round(end-start)) + \" seconds\")\n",
    "    after = mt.count_cols()\n",
    "    n_cases_after = mt.filter_cols(mt.is_case).count_cols()\n",
    "    print(\"After IBD pruning \" + str(after) + \"/\" + str(before) + \" samples, including \" + str(n_cases_after) + \"/\" + str(n_cases_before) + \" cases, remain.\")\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation with Hail annotation database\n",
    "[Hail Documentation for Annotation Database](https://hail.is/docs/0.2/annotation_database_ui.html#id1 )\n",
    "\n",
    "Only possible on Google Cloud Platform   \n",
    "Only available with latest Hail version (>0.2.34) therefore not directly compatible with VariantSpark in current version as it is dependent on Hail 0.2.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def remove_empty_unpack(mt):\n",
    "    mt = mt.filter_rows(hl.len(mt.gencode.gene_name)>0)\n",
    "    mt = mt.filter_rows(hl.len(mt.gencode.gene_id)>0)\n",
    "    mt = mt.annotate_rows(gene_name1 = mt.gencode.gene_name[0])\n",
    "    mt = mt.annotate_rows(gene_id1 = mt.gencode.gene_id[0])\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hail annotation database to annotate genes from gencode\n",
    "def annotate_genes(mt, ht):\n",
    "    db = hl.experimental.DB()\n",
    "    mt = db.annotate_rows_db(mt,\"gencode\") \n",
    "    ht = mt.select_rows(mt.gencode).rows()\n",
    "    #mt = mt.annotate_rows(gencode = ht[mt.locus, mt.alleles].gencode)\n",
    "    # filter out SNPs without annotation and reduce annotation to one gene name\n",
    "    mt = remove_empty_unpack(mt)\n",
    "    return mt, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for burden test: use only top n SNPs\n",
    "def get_top_burden(mt, ht, method=\"GWAS\", top_n=1000):    \n",
    "    if method == \"GWAS\":\n",
    "        ht = ht.order_by(hl.asc('p_value')).add_index(name='new_rank')\n",
    "    elif method==\"VS\":\n",
    "        ht = ht.order_by(hl.desc('importance')).add_index(name='new_rank')\n",
    "    else:\n",
    "        raise(\"Use method='GWAS' or method='VS'\")\n",
    "    ht = ht.filter(ht.new_rank<top_n, keep=True)\n",
    "    ht = ht.key_by(\"locus\", \"alleles\")\n",
    "    mt_top = mt.semi_join_rows(ht)\n",
    "    return mt, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GWAS\n",
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform logistic regression on the given matrix table\n",
    "# use this for only 1 control, 1 sample group, annotated with is_case\n",
    "# needs previously performed PCA annotation with (at least) 5 PCs\n",
    "def gwas_log_regression_1c(mt):\n",
    "    # covariates = first 5 PCs\n",
    "    print(\"GWAS running\")\n",
    "    gwas_ht = hl.logistic_regression_rows(\n",
    "                             test='wald',\n",
    "                             y=mt.is_case,\n",
    "                             x=mt.GT.n_alt_alleles(),\n",
    "                             covariates=[1.0, mt.is_female, mt.scores.scores[0], mt.scores.scores[1], mt.scores.scores[2], mt.scores.scores[3], mt.scores.scores[4]])\n",
    "    # order top snps by pvalue and add log10(p) as column to table\n",
    "    gwas_ht = gwas_ht.annotate(log10= -hl.log10(gwas_ht.p_value)).order_by(hl.asc('p_value')).add_index(name='gwas_rank')\n",
    "    # add key for being able to annotate mt\n",
    "    # this erases ordering (doesn't matter since loci will be annotated anyways)\n",
    "    gwas_ht = gwas_ht.key_by('locus', 'alleles')\n",
    "    # annotate mt with gwas results: pval and -log10(pval)\n",
    "    mt = mt.annotate_rows(log_reg_pval = gwas_ht[mt.locus, mt.alleles].p_value)\n",
    "    mt = mt.annotate_rows(log_reg_log10p = gwas_ht[mt.locus, mt.alleles].log10)\n",
    "    mt = mt.annotate_rows(log_reg_rank = gwas_ht[mt.locus, mt.alleles].gwas_rank)\n",
    "    \n",
    "    return gwas_ht, mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints QQ and manhattan plot\n",
    "def qq_manh_plot(gwas_ht, title=\"Asthma\", sign_threshold=5e-8):\n",
    "    print(\">>>>> qq and manhattan plot\")\n",
    "    # check if GWAS was well controlled with qq plot (assess inflation)\n",
    "    p_qq = hl.plot.qq(gwas_ht.p_value, title=title)\n",
    "    show(p_qq)\n",
    "    # visualize significant SNPs with manhattan plot\n",
    "    p_manhattan = hl.plot.manhattan(gwas_ht.p_value, title=\"Manhattan plot of \" + title + \", logistic regression\", significance_line=sign_threshold, collect_all=True)\n",
    "    show(p_manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variant Spark\n",
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Variant Spark AFTER logistic regression\n",
    "def vs_rf_after_gwas(mt, gwas, n_trees=5000, mTry=0.1, seed=4957):\n",
    "    print(\">>>>> Start VariantSpark\")\n",
    "    # since Variant Spark cannot build model on data with missing genotypes (throws NullPointerException): \n",
    "    # filter out all SNPs with missing genotypes\n",
    "    #c1s_mt.aggregate_entries(hl.agg.stats(c1s_mt.variant_qc.call_rate))\n",
    "    mt = hl.sample_qc(mt)\n",
    "    mt = hl.variant_qc(mt)\n",
    "    mt_no_missing_genotypes = mt.filter_rows(mt.variant_qc.call_rate == 1, keep= True)\n",
    "    before = mt.count_rows()\n",
    "    after = mt_no_missing_genotypes.count_rows()\n",
    "    numFeatures=after\n",
    "    print(\"Filtered \" + str(before-after)+ \"/\" + str(before) + \" variants with missing genotypes (required for VS)\")\n",
    "\n",
    "    start = time.time()\n",
    "    rf_model = vshl.random_forest_model(y=mt_no_missing_genotypes.is_case,\n",
    "                    x=mt_no_missing_genotypes.GT.n_alt_alleles(),\n",
    "                    mtry_fraction=mTry,\n",
    "                    max_depth=5,\n",
    "                    min_node_size = 200,\n",
    "                    seed=seed)\n",
    "    rf_model.fit_trees(n_trees=n_trees, batch_size = 100) \n",
    "    end = time.time()\n",
    "    print(\"Building VS random forest model took \" + str(round(end-start)) + \" seconds\")\n",
    "    print(\"rf_model.oob error=\"+str(rf_model.oob_error()))\n",
    "    impTable = rf_model.variable_importance()\n",
    "    gwas_with_imp = gwas.join(impTable)\n",
    "    # add a rank to make finding top SNPs easier\n",
    "    gwas_with_imp = gwas_with_imp.order_by(hl.desc('importance')).add_index(name='vs_rank')\n",
    "    gwas_with_imp = gwas_with_imp.key_by('locus', 'alleles') \n",
    "    mt = mt.annotate_rows(vs_imp = gwas_with_imp[mt.locus, mt.alleles].importance) \n",
    "    mt = mt.annotate_rows(vs_rank = gwas_with_imp[mt.locus, mt.alleles].vs_rank) \n",
    "    return gwas_with_imp, mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vs_manhattan(gwas_with_imp, title=\"Asthma\"):\n",
    "    p = vshlplt.manhattan_imp(gwas_with_imp.importance, \n",
    "                            significance_line = None, title=\"Manhattan plot of \" +title+\", random forest\")\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burden testing\n",
    "[Documentation - SKAT](https://hail.is/docs/0.2/methods/genetics.html#hail.methods.skat)\n",
    "[Discussion board - Burden](https://discuss.hail.is/t/logistic-regression-burden-tests/206/3)   \n",
    "Requires gene annotations with the annotation function and reduction of the table with get_top_burden   \n",
    "Because of compatibility issues (GCP vs AWS, required Hail version vs current VariantSpark Hail version) this is not part of the <a href='#pipeline'>pipeline</a>, but can be performed in different notebooks/other platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skat(mt):\n",
    "    mt = hl.variant_qc(mt)\n",
    "    mt = mt.annotate_rows(weight = hl.dbeta(hl.min(mt.variant_qc.AF), 1.0, 25.0) ** 2)\n",
    "    skat_table = hl.skat(key_expr=mt.gene_name1,\n",
    "                weight_expr=mt.weight,\n",
    "                y=mt.is_case,\n",
    "                x=mt.GT.n_alt_alleles(),\n",
    "                covariates=[1.0, mt.is_female, mt.scores.scores[0], mt.scores.scores[1], mt.scores.scores[2], mt.scores.scores[3], mt.scores.scores[4]],\n",
    "                    logistic=True,\n",
    "                    iterations=100000)\n",
    "    print(\"Fraction of results with no issues: \" + str(round(skat_table.aggregate(hl.agg.fraction(skat_table.fault == 0)), 2)))\n",
    "    return skat_table\n",
    "# skat_table.order_by(skat_table.p_value).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def burden(mt):\n",
    "    gene_mt = mt.group_rows_by(mt.gene_name1).aggregate(\n",
    "    mac = hl.agg.sum(\n",
    "        hl.cond(mt.variant_qc.AF[1] <= 0.5,\n",
    "                mt.GT.n_alt_alleles(),\n",
    "                2 - mt.GT.n_alt_alleles())))\n",
    "    gene_mt = hl.logistic_regression_rows(\n",
    "    y=gene_mt.is_case, \n",
    "    x=gene_mt.mac,\n",
    "    covariates=[1.0, gene_mt.is_female, gene_mt.scores.scores[0], gene_mt.scores.scores[1], gene_mt.scores.scores[2], gene_mt.scores.scores[3], gene_mt.scores.scores[4]],\n",
    "    test='wald')\n",
    "    return gene_mt\n",
    "# gene_mt.order_by(gene_mt.p_value).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of GWAS and VS Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Variant Spark with GWAS logistic regression results\n",
    "Scatter plot of sqrt(random forest importance score) vs -log10(logistic regression p-value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_scatter(gwas_with_imp, title=\"Asthma\"):\n",
    "    gwas_with_imp = gwas_with_imp.annotate(sqrt_importance = hl.sqrt(gwas_with_imp[\"importance\"]))\n",
    "    scatter= hl.plot.scatter(x=gwas_with_imp[\"log10\"], y=gwas_with_imp[\"sqrt_importance\"],\n",
    "                         xlabel= \"-log10(Hail P-value)\",\n",
    "                         ylabel= \"sqrt(VariantSpark Importance)\",\n",
    "                         title=\"Comparing GWAS with Variant Spark (\"+ title + \")\")\n",
    "    show(scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benjamini Hochberg \n",
    "p-value adjustment for multiple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: can cause timeouts\n",
    "# return df with variants and their respective Benjamini Hochberg adjusted pvalue\n",
    "\n",
    "def bh_thresh(mt, gwas_ht, fdr=0.05):\n",
    "    print(\">>>>> significance with benjamini hochberg threshold\")\n",
    "    \n",
    "    mt = mt.annotate_rows(log_reg_pval = gwas_ht[mt.locus, mt.alleles].p_value)\n",
    "    mt = mt.filter_rows(hl.is_missing(mt.log_reg_pval), keep=False)\n",
    "    mt = mt.annotate_rows(log_reg_rank = gwas_ht[mt.locus, mt.alleles].gwas_rank)\n",
    "    # work around bug: instead of directly annotating bh_thresh in hail (errors), write out as pandas, read back in and calculate bh_thresh\n",
    "    table = mt.rows()\n",
    "    table = table.select(table.log_reg_pval, table.log_reg_rank)\n",
    "    print(\"exporting table\")\n",
    "    table.export(hl_path+'test.tsv')\n",
    "    print(\"importing tsv\")\n",
    "    df = pd.read_csv(hl_path+'test.tsv', sep='\\t')\n",
    "    print(\"calculate bh threshold\")\n",
    "    n_var = df['log_reg_pval'].count()\n",
    "    df['bh_thresh'] = df.log_reg_pval < (((df.log_reg_rank+1)/n_var)*fdr)\n",
    "    print(\"Variants passing bh threshold\")\n",
    "    print(df.loc[df['bh_thresh']==True][['locus','alleles','log_reg_pval','log_reg_rank','bh_thresh' ]])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlaps with SNPs associated with the phenotype from GWAS catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GWAS catalog file to interval table usable with hail\n",
    "# This function is untested on all but 3 GWAS catalog files (asthma, type I diabetes, height) and prone to errors\n",
    "\n",
    "# delta is the window around the locus from GWAS catalog. So 50,000 would mean +/- 25,000 bp around locus\n",
    "def gwas_catalog_to_intervals(gwas_catalog_filepath, delta = 50000):\n",
    "    gwas_cat_df = pd.read_csv(gwas_catalog_filepath, sep=\"\\t\")\n",
    "    gwas_cat_df.sort_values(by=['P-VALUE'], inplace = True)\n",
    "    gwas_cat_df.drop(axis=1, labels=['DATE ADDED TO CATALOG', 'PUBMEDID', 'FIRST AUTHOR', 'DATE', 'JOURNAL',\n",
    "       'LINK', 'STUDY', 'CONTEXT', 'INTERGENIC', 'INITIAL SAMPLE SIZE', 'REPLICATION SAMPLE SIZE',\n",
    "        'UPSTREAM_GENE_ID', 'DOWNSTREAM_GENE_ID', 'UPSTREAM_GENE_DISTANCE', 'DOWNSTREAM_GENE_DISTANCE', 'MERGED', 'CNV',\n",
    "        'P-VALUE (TEXT)', 'MAPPED_TRAIT_URI', 'STUDY ACCESSION', 'OR or BETA', '95% CI (TEXT)', 'PLATFORM [SNPS PASSING QC]',\n",
    "       'GENOTYPING TECHNOLOGY'], inplace=True)\n",
    "    gwas_cat_df[\"locus\"] = [\"chr\"+str(gwas_cat_df[\"CHR_ID\"][i])+\":\"+str(gwas_cat_df[\"CHR_POS\"][i]) for i in range(gwas_cat_df[\"CHR_ID\"].size)]\n",
    "    gwas_cat_df.dropna(subset=[\"CHR_ID\", \"CHR_POS\"], inplace=True)\n",
    "    gwas_cat_df =gwas_cat_df.dropna(axis=1)\n",
    "    gwas_cat_df.to_csv(hl_path+\"gwas_catalog_loci.tsv\",columns=[\"locus\"], sep=\"\\t\", index=False)\n",
    "    gwas_cat_df = gwas_cat_df.astype({'CHR_POS': 'int32'})\n",
    "    interval = {}\n",
    "    for i in range(len(gwas_cat_df[\"CHR_POS\"])):\n",
    "        try:\n",
    "            s = \"chr\"+str(gwas_cat_df[\"CHR_ID\"][i])+\":\"+str(gwas_cat_df[\"CHR_POS\"][i]-delta)+\"-\"+str(gwas_cat_df[\"CHR_POS\"][i]+delta)\n",
    "            interval.update({i:s})\n",
    "        except KeyError:\n",
    "            pass\n",
    "    gwas_cat_df[\"interval\"] = pd.Series(interval)\n",
    "    gwas_cat_df.dropna(subset=[\"interval\"], inplace=True)\n",
    "    gwas_cat_df.to_csv(hl_path+\"gwas_catalog_intervals_\"+str(delta)+\"_windows.tsv\",columns=[\"interval\"] , sep=\"\\t\", index=False, header=False)\n",
    "    # format for intervals that includes annotation: contig  start  end  direction  target\n",
    "    gwas_cat_df[\"chr\"] = [\"chr\"+str(gwas_cat_df[\"CHR_ID\"][i]) for i in range(gwas_cat_df[\"CHR_ID\"].size)]\n",
    "    gwas_cat_df[\"interval_start\"] = [str(gwas_cat_df[\"CHR_POS\"][i]-delta) for i in range(gwas_cat_df[\"CHR_ID\"].size)]\n",
    "    gwas_cat_df[\"interval_end\"] = [str(gwas_cat_df[\"CHR_POS\"][i]+delta) for i in range(gwas_cat_df[\"CHR_ID\"].size)]\n",
    "    gwas_cat_df[\"dir\"] = [\"-\" for i in range(gwas_cat_df[\"CHR_ID\"].size)]\n",
    "    gwas_cat_df.to_csv(hl_path+\"gwas_catalog_intervals_\"+str(delta)+\"_windows_gene.tsv\",columns=[\"chr\",\"interval_start\", \"interval_end\", \"dir\", \"REPORTED GENE(S)\"] , sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any of the the top x SNPs have overlap with GWAS catalog SNPs\n",
    "def check_for_overlaps_in_top_snps(gwas_with_imp, gwas_ht, top_n=500, ref_genome='GRCh38', interval_path=hl_path+\"intervals_50000.tsv\"):\n",
    "    print(\">>>>> Check for overlaps with GWAS catalog SNPs (reported as significant by other studies) with top \" + str(top_n) + \" highest ranked SNPs\")\n",
    "\n",
    "    # get top_n ranked in gwas - use gwas_with_imp.rank\n",
    "    print(\"top_n = \" + str(top_n))\n",
    "    top_gwas_snps = gwas_ht.filter(gwas_ht.gwas_rank < top_n, keep=True)\n",
    "    print(\"# GWAS SNPs: \" + str(top_gwas_snps.count()))\n",
    "    # get top_n importance scores\n",
    "    top_vs_snps = gwas_with_imp.filter(gwas_with_imp.vs_rank < top_n, keep=True)\n",
    "    print(\"# VS SNPs: \" + str(top_vs_snps.count()))\n",
    "\n",
    "    # SNPs found with both methods\n",
    "    top_both_snps = top_vs_snps.filter(top_vs_snps.gwas_rank < top_n, keep=True)\n",
    "    print(\"# both SNPs: \" + str(top_both_snps.count()))\n",
    " \n",
    "    # check for overlaps with GWAS catalog \n",
    "    intervals = hl.import_locus_intervals(interval_path, reference_genome=ref_genome)\n",
    "    \n",
    "    # annotate and filter, then count and output\n",
    "    top_gwas_snps = top_gwas_snps.annotate(region = hl.is_defined(intervals[top_gwas_snps.locus]))\n",
    "    top_vs_snps = top_vs_snps.annotate(region = hl.is_defined(intervals[top_vs_snps.locus]))\n",
    "    top_both_snps = top_both_snps.annotate(region = hl.is_defined(intervals[top_both_snps.locus]))\n",
    "    top_gwas_snps_region =  top_gwas_snps.filter(top_gwas_snps.region)\n",
    "    top_vs_snps_region =  top_vs_snps.filter(top_vs_snps.region)\n",
    "    top_both_snps_region =  top_both_snps.filter(top_both_snps.region)\n",
    "    print(\"Of top \" + str(top_n) + \" SNPs from GWAS \" + str(top_gwas_snps_region.count()) + \" were within \"+str(window_size)+\"bp of a GWAS catalog SNP.\")\n",
    "    print(\"Of top \" + str(top_n) + \" SNPs from VS \" + str(top_vs_snps_region.count()) + \" were within \"+str(window_size)+\"bp of GWAS catalog SNP.\")\n",
    "    print(\"Of SNPs in top \" + str(top_n) + \" with both GWAS and VS (overlap: \"+str(top_both_snps.count())+\") \" + str(top_both_snps_region.count()) + \" were within \"+str(window_size)+\"bp of a GWAS catalog SNP.\")\n",
    "    return top_gwas_snps_region, top_vs_snps_region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polygenic Risk Score\n",
    "[Hail Documentation on PRS](https://hail.is/docs/0.2/guides/genetics.html#polygenic-risk-score-calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and annotate polygenic risk scores to each sample\n",
    "# based on beta from logistic regression or importance score in VS\n",
    "# NOTE: beta represents the direction that the phenotype is influenced in (pos./neg.) by each SNP while importance score does not\n",
    "# therefore PRS with importance score is not useful. (Feature contribution would be the counterpart of beta for random forest)\n",
    "def prs(mt, method=\"gwas\"):\n",
    "    mt = hl.variant_qc(mt)\n",
    "    mt = mt.annotate_rows(MAF = hl.min(mt.variant_qc.AF))\n",
    "    # assuming 'allele' is the alternate allele with lower AF\n",
    "    mt = mt.annotate_rows(allele=(hl.cond(mt.MAF == mt.variant_qc.AF[0], mt.alleles[0], mt.alleles[1])))\n",
    "    flip = hl.case().when(mt.allele == mt.alleles[0], True).when(mt.allele == mt.alleles[1], False).or_missing()\n",
    "    mt = mt.annotate_rows(flip=flip)\n",
    "    mt = mt.annotate_rows(prior=2 * hl.cond(mt.flip, mt.variant_qc.AF[0], mt.variant_qc.AF[1]))\n",
    "    # assuming 'score' is beta from log. reg.\n",
    "    if method==\"gwas\":\n",
    "        score = mt.beta\n",
    "    elif method==\"vs\":\n",
    "        score=mt.importance\n",
    "    mt = mt.annotate_cols(prs=hl.agg.sum(\n",
    "        score *\n",
    "            hl.coalesce(\n",
    "                hl.cond(\n",
    "                mt.flip, 2 - mt.GT.n_alt_alleles(),\n",
    "                mt.GT.n_alt_alleles()), \n",
    "            mt.prior)))\n",
    "    return mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction accuracy with PRS on training and test data\n",
    "# p>0.5 means more samples in training set, p=0.5 means approximately 50/50 split\n",
    "# WARNING: have experienced timeouts in this function. If timeout occurs, restart kernel\n",
    "def pred_accuracy_prs(mt, p=0.5):\n",
    "    training_mt, test_mt = test_training_split(mt, p=0.5)\n",
    "    gwas_ht, training_mt = gwas_log_regression_1c(training_mt)\n",
    "    \n",
    "    training_mt = training_mt.annotate_rows(beta = gwas_ht[training_mt.locus, training_mt.alleles].beta)\n",
    "    training_mt = prs(training_mt, method=\"gwas\")\n",
    "    training_mt = training_mt.annotate_cols(prs_pred = training_mt.prs>0)\n",
    "    # number of samples predicted correctly by PRS = number of samples where is_case and prs_pred is the same\n",
    "    tmp = training_mt.filter_cols(training_mt.is_case)\n",
    "    tmp = tmp.filter_cols(tmp.prs_pred)\n",
    "    true_pred = tmp.count_cols()\n",
    "    # true_pred + all the correct negative predictions!\n",
    "    tmp = training_mt.filter_cols(~training_mt.is_case)\n",
    "    tmp = tmp.filter_cols(~tmp.prs_pred)\n",
    "    true_pred = true_pred + tmp.count_cols()\n",
    "    false_pred = training_mt.count_cols() - true_pred\n",
    "    acc = true_pred/(true_pred+false_pred)\n",
    "    print(\"PRS accuracy on training set: \" + str(acc))\n",
    "    \n",
    "    test_mt = test_mt.annotate_rows(beta = training_mt.index_rows(test_mt.row_key).beta)\n",
    "    test_mt = prs(test_mt, method=\"gwas\")\n",
    "    test_mt = test_mt.annotate_cols(prs_pred = test_mt.prs>0)\n",
    "    true_pred = test_mt.filter_cols(test_mt.prs_pred == test_mt.is_case).count_cols()\n",
    "    false_pred = test_mt.count_cols() - true_pred\n",
    "    acc = true_pred/(true_pred+false_pred)\n",
    "    print(\"PRS accuracy on test set: \" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Accuracy with random forest on top SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if MAF, Call rate, HWE is different in cases vs controls, which may confound random forest results\n",
    "# (this method only makes sense when using only the TOP SNPs from VS)\n",
    "def noise_pred(mt, top_snps_ht, method=\"GWAS\"):\n",
    "    mt_top = mt.semi_join_rows(top_snp_ht)\n",
    "    mt_top_cases = mt_top.filter_cols(mt_top.is_case)\n",
    "    mt_top_cases = hl.variant_qc(mt_top_cases)\n",
    "    mt_top_controls = mt_top.filter_cols(~mt_top.is_case)\n",
    "    mt_top_controls = hl.variant_qc(mt_top_controls)\n",
    "    print(\"MAF\")\n",
    "    print(\"Cases stats\")\n",
    "    print(mt_top_cases.aggregate_entries(hl.agg.stats(mt_top_cases.MAF)))\n",
    "    print(\"Controls stats\")\n",
    "    print(mt_top_controls.aggregate_entries(hl.agg.stats(mt_top_controls.MAF)))\n",
    "    p = hl.plot.histogram(mt_top_controls.MAF, title=\"MAF - \" + method +\" controls\")\n",
    "    show(p)\n",
    "    p = hl.plot.histogram(mt_top_cases.MAF, title=\"MAF - \" + method +\" cases\")\n",
    "    show(p)\n",
    "    print(\"Call rate\")\n",
    "    print(\"Cases stats\")\n",
    "    print(mt_top_cases.aggregate_entries(hl.agg.stats(mt_top_cases.variant_qc.call_rate)))\n",
    "    print(\"Controls stats\")\n",
    "    print(mt_top_controls.aggregate_entries(hl.agg.stats(mt_top_controls.variant_qc.call_rate)))\n",
    "    print(\"Hardy Weinberg\")\n",
    "    print(\"Cases stats\")\n",
    "    print(mt_top_cases.aggregate_entries(hl.agg.stats(mt_top_cases.variant_qc.p_value_hwe)))\n",
    "    print(\"Controls stats\")\n",
    "    print(mt_top_controls.aggregate_entries(hl.agg.stats(mt_top_controls.variant_qc.p_value_hwe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test and training set\n",
    "def test_training_split(mt, p=0.5, seed=29347):\n",
    "    print(\"Total number of samples: \" + str(mt.count_cols()))\n",
    "    cases_mt = mt.filter_cols(mt.is_case)\n",
    "    print(\"Number of cases: \" + str(cases_mt.count_cols()))\n",
    "    controls_mt = mt.filter_cols(~mt.is_case)\n",
    "    print(\"Number of controls: \" + str(controls_mt.count_cols()))\n",
    "    print(\"Split with p=\" + str(p))\n",
    "    training_cases_mt = cases_mt.sample_cols(p=p, seed=seed)\n",
    "    training_controls_mt = controls_mt.sample_cols(p=p, seed=seed)\n",
    "    print(\"Training set contains \" + str(training_cases_mt.count_cols()) + \" cases and \" + str(training_controls_mt.count_cols()) + \" controls.\")\n",
    "    test_cases_mt = cases_mt.anti_join_cols(training_cases_mt.cols())\n",
    "    test_controls_mt = controls_mt.anti_join_cols(training_controls_mt.cols())\n",
    "    training_mt = training_cases_mt.union_cols(training_controls_mt)\n",
    "    test_mt = test_cases_mt.union_cols(test_controls_mt)\n",
    "    return training_mt, test_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top n most significant/important SNPs\n",
    "def get_top(ht, method=\"GWAS\", top_n=500):    \n",
    "    if method == \"GWAS\":\n",
    "        ht = ht.order_by(hl.asc('p_value')).add_index(name='new_rank')\n",
    "    elif method==\"VS\":\n",
    "        ht = ht.order_by(hl.desc('importance')).add_index(name='new_rank')\n",
    "    else:\n",
    "        raise(\"Use method='GWAS' or method='VS'\")\n",
    "    ht = ht.filter(ht.new_rank<top_n, keep=True)\n",
    "    ht = ht.key_by(\"locus\", \"alleles\")\n",
    "    return ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive labels from hail matrix table and convert to numpy array (compatibility with sklearn)\n",
    "def get_labels(mt):\n",
    "    # get labels as np array\n",
    "    mt = mt.select_cols(mt.is_case) \n",
    "    # preserve order\n",
    "    mt = mt.key_cols_by()\n",
    "    t = mt.cols() \n",
    "    t.export(hl_path+\"tmp/table2.csv\",delimiter=',')\n",
    "    pd_t = pd.read_csv(hl_path+\"tmp/table2.csv\")\n",
    "    labels = np.array(pd_t.is_case)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive features from hail matrix table and convert to numpy array (compatibility with sklearn)\n",
    "def get_features(mt):\n",
    "    # reduce mt to only entries\n",
    "    mt = mt.select_cols()\n",
    "    mt = mt.select_rows()\n",
    "    # make table with rows locus, alleles, sample.GT for each sample\n",
    "    table = mt.make_table()\n",
    "    # to_pandas (does not work for large dataframes)\n",
    "    #pd_t = table.to_pandas()\n",
    "    table.export(hl_path+\"tmp/table1.csv\",delimiter=',')\n",
    "    pd_t = pd.read_csv(hl_path+\"tmp/table1.csv\", dtype=\"str\")\n",
    "    pd_t.fillna(value='0/0', inplace=True) \n",
    "    replace_dict = {'0/0': 0, '0/1': 1, '1/1': 2}\n",
    "    pd_t = pd_t.replace(replace_dict)\n",
    "    pd_t = pd_t.drop(labels= ['locus', 'alleles'], axis=1)\n",
    "    pd_t = pd_t.applymap(int)\n",
    "    pd_t = pd_t.transpose()\n",
    "    features = np.array(pd_t.values)\n",
    "    features = features.astype(int)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for get_features and get_labels\n",
    "def get_features_labels(mt, top_snps_ht, prnt = \"get features and labels\"):\n",
    "    print(prnt)\n",
    "    mt_top = mt.semi_join_rows(top_snps_ht)\n",
    "    mt_top = replace_empty_gt(mt_top)\n",
    "    labels = get_labels(mt_top)\n",
    "    features = get_features(mt_top)\n",
    "    print(\"features.shape: \" + str(features.shape))\n",
    "    print(\"len(labels): \" + str(len(labels)))\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output accuracy, ROC curve, ROC AUC\n",
    "def pred_acc_auc(rf, predictions, test_labels, test_features, method='GWAS'):\n",
    "    #Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    false_pred = sum([abs(x) for x in test_labels - predictions])\n",
    "    true_pred = len(predictions)-false_pred\n",
    "    acc = true_pred/(true_pred+false_pred)\n",
    "    print('Accuracy:', round(acc*100,2), '%')\n",
    "    # Percentage predicted positive\n",
    "    print(\"Predicted as case: \", round(100*sum(predictions)/len(predictions),2), '%')\n",
    "    # auc\n",
    "    predictions_prob = rf.predict_proba(test_features)\n",
    "    roc_auc = roc_auc_score(test_labels, predictions_prob[:,1])\n",
    "    print('ROC AUC score: ' + str(round(100*roc_auc,2)))\n",
    "    # ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true = test_labels, y_score = predictions_prob[:,1], pos_label=1)\n",
    "    roc_auc= auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic ' + method)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform GWAS with logistic regression and random forest with n_trees trees \n",
    "# and get prediction accuracy of a new random forest with the top_n most significant/important SNPs from GWAS.\n",
    "# adjust p for training/test set size\n",
    "def prediction_accuracy(mt, top_n=200, p=0.5, seed=59172, n_trees=1000):\n",
    "    # step 1: separate cases and controls\n",
    "    cases_mt = mt.filter_cols(mt.is_case)\n",
    "    controls_mt = mt.filter_cols(~mt.is_case)\n",
    " \n",
    "    # step 2: downsample with sample_cols(0.5) to form training set\n",
    "    training_cases_mt = cases_mt.sample_cols(p=p, seed=seed) # keep each column with probability p\n",
    "    training_controls_mt = controls_mt.sample_cols(p=p, seed=seed)\n",
    "    \n",
    "    # step 3: get the eids of training to exclude from mt to form test set\n",
    "    test_cases_mt = cases_mt.anti_join_cols(training_cases_mt.cols())\n",
    "    test_controls_mt = controls_mt.anti_join_cols(training_controls_mt.cols())\n",
    "    \n",
    "    # step 4: perform joins to merge cases and controls of training and test set\n",
    "    training_mt = training_cases_mt.union_cols(training_controls_mt)\n",
    "    test_mt = test_cases_mt.union_cols(test_controls_mt)\n",
    "    \n",
    "    # check number of cases and controls in training and test set\n",
    "    print(\"Number of cases in training_mt: \" + str(training_mt.filter_cols(training_mt.is_case).count_cols()))\n",
    "    print(\"Number of controls in training_mt: \" + str(training_mt.filter_cols(~training_mt.is_case).count_cols()))\n",
    "    print(\"Number of cases in test_mt: \" + str(test_mt.filter_cols(test_mt.is_case).count_cols()))\n",
    "    print(\"Number of controls in test_mt: \" + str(test_mt.filter_cols(~test_mt.is_case).count_cols()))\n",
    "\n",
    "    \n",
    "    # step 5: do GWAS and VS on the training set\n",
    "    gwas_ht, training_mt = gwas_log_regression_1c(training_mt)\n",
    "    gwas_with_imp, training_mt = vs_rf_after_gwas(training_mt, gwas_ht, n_trees=n_trees) # call rate == 1 filter\n",
    "\n",
    "    # step 6: get top n SNPs to train the random forest with \n",
    "    gwas_top = get_top(gwas_ht, method=\"GWAS\", top_n=top_n)\n",
    "    vs_top = get_top(gwas_with_imp, method=\"VS\", top_n = top_n)\n",
    "    \n",
    "    # step 5.5: how different are the top_n (lowest p, highest imp) SNPs\n",
    "    print(\"Out of top_n=\"+str(top_n)+\" SNPs, \"+str(gwas_top.semi_join(vs_top).count()) + \" are identical.\\nNote: Discrepancy could be bc of call_rate==1 filter for VS.\")\n",
    "    training_mt_top_gwas = training_mt.semi_join_rows(gwas_top)\n",
    "    test_mt_top_gwas = test_mt.semi_join_rows(gwas_top)\n",
    "    training_mt_top_vs = training_mt.semi_join_rows(vs_top)\n",
    "    test_mt_top_vs = test_mt.semi_join_rows(vs_top)\n",
    "    \n",
    "    # step 6.5: check noise prediction (MAF/Call rate/HWE of the top SNPs predicted by VS in cases vs controls)\n",
    "    #noise_pred(training_mt, training_mt_top_vs)\n",
    "    \n",
    "    # step 7: get features and labels for these SNPs in training and test dataset\n",
    "    training_labels_gwas, training_features_gwas = get_features_labels(training_mt_top_gwas, gwas_top, prnt=\"train gwas\")\n",
    "    print(\"Number of pos. labels in training gwas: \" + str(sum(training_labels_gwas)))\n",
    "    training_labels_vs, training_features_vs = get_features_labels(training_mt_top_vs, vs_top, prnt=\"train vs\")\n",
    "    print(\"Number of pos. labels in training vs: \" + str(sum(training_labels_vs)))\n",
    "    test_labels_gwas, test_features_gwas = get_features_labels(test_mt_top_gwas, gwas_top, prnt=\"test gwas\")\n",
    "    print(\"Number of pos. labels in test gwas: \" + str(sum(test_labels_gwas)))\n",
    "    test_labels_vs, test_features_vs = get_features_labels(test_mt_top_vs, vs_top, prnt=\"test vs\")\n",
    "    print(\"Number of pos. labels in test vs: \" + str(sum(test_labels_vs)))\n",
    "    \n",
    "    # step 8: train the random forests with top snps from GWAS and VS\n",
    "    rf_gwas = RandomForestClassifier(n_estimators = 1000, random_state = 472, max_features = \"sqrt\", max_depth=60, min_samples_split=10)\n",
    "    rf_vs = RandomForestClassifier(n_estimators = 1000, random_state = 472, max_features = \"sqrt\", max_depth=60, min_samples_split=10)\n",
    "    rf_gwas.fit(training_features_gwas, training_labels_gwas);\n",
    "    rf_vs.fit(training_features_vs, training_labels_vs);\n",
    "    \n",
    "    # step 9: predict the test labels\n",
    "    gwas_predictions = rf_gwas.predict(test_features_gwas)\n",
    "    gwas_self_predictions = rf_gwas.predict(training_features_gwas)\n",
    "    vs_predictions = rf_vs.predict(test_features_vs)\n",
    "    vs_self_predictions = rf_vs.predict(training_features_vs)\n",
    "    \n",
    "    # step 10: accuracy, ROC AUC\n",
    "    print(\"GWAS Accuracy - On training data\")\n",
    "    pred_acc_auc(rf=rf_gwas, predictions=gwas_self_predictions, test_labels=training_labels_gwas, test_features=training_features_gwas, method='GWAS')\n",
    "    print(\"GWAS Accuracy - On test data\")\n",
    "    pred_acc_auc(rf=rf_gwas, predictions=gwas_predictions, test_labels=test_labels_gwas, test_features=test_features_gwas, method='GWAS')\n",
    "    #pred_acc_auc(rf_gwas, gwas_predictions, test_labels_gwas, test_features_gwas, method='GWAS')\n",
    "    print(\"VS Accuracy - On training data\")\n",
    "    pred_acc_auc(rf=rf_vs, predictions=vs_self_predictions, test_labels= training_labels_vs, test_features=training_features_vs, method='VS')\n",
    "    print(\"VS Accuracy - On test data\")\n",
    "    pred_acc_auc(rf=rf_vs, predictions=vs_predictions, test_labels= test_labels_vs, test_features=test_features_vs, method='VS')\n",
    "    \n",
    "    # step 11: look at probabilities \n",
    "    print(\"Plot proba on GWAS\")\n",
    "    plot_proba(rf_gwas, test_features_gwas, training_features_gwas, test_labels_gwas, training_labels_gwas)\n",
    "    print(\"Plot proba on VS\")\n",
    "    plot_proba(rf_vs, test_features_vs, training_features_vs, test_labels_vs, training_labels_vs)\n",
    "    \n",
    "    # step 12: plot a tree\n",
    "    # Extract single tree\n",
    "    estimator = rf_vs.estimators_[10]\n",
    "    fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (25,4), dpi=300)\n",
    "    \n",
    "    tree.plot_tree(estimator,\n",
    "    #           feature_names = fn, \n",
    "    #           class_names=cn,\n",
    "               filled = True,\n",
    "                  fontsize=5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for hyperparameter settings\n",
    "To optimize hyperparameters of the prediction random forest, set parameters in the param_grid in my_grid_search   \n",
    "To test the improvement the best_param settings bring in comparison to default parameters (or previous param settings) use evaluate_new_params    \n",
    "Uses [Scikit Learn RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_grid_search(mt, features, labels):\n",
    "    param_grid = {\n",
    "        'bootstrap': [False],\n",
    "        'max_depth': [10, 40, 90],\n",
    "        'max_features': [0.3, 0.5, 0.7], \n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'n_estimators': [1000, 2500] \n",
    "        }\n",
    "    rf = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, n_iter = 100, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n",
    "    rf_random.fit(features, labels)\n",
    "    print(\"best param\")\n",
    "    best_param = rf_random.best_params_\n",
    "    print(best_param)\n",
    "    print(\"best score: \" + str(round(rf_random.best_score_, 4)))\n",
    "    cv_res = pd.DataFrame.from_dict(rf_random.cv_results_)\n",
    "    cv_res.to_csv(\"rf_cv_results.csv\", index=False)\n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_new_params(best_params, features, labels):\n",
    "    X = features\n",
    "    y = labels\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test \n",
    "    base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "    base_model.fit(X_train, y_train)\n",
    "    print(\"base model\")\n",
    "    base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "    #best_random = rf_random.best_estimator_\n",
    "    best_random = RandomForestClassifier(**best_params) \n",
    "    best_random.fit(X_train, y_train)\n",
    "    print(\"best param\")\n",
    "    random_accuracy = evaluate(best_random, X_test, y_test)\n",
    "\n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of genes \n",
    "i.e. for [GO analysis](http://geneontology.org/)   \n",
    "Circumvents need for GCP Hail annotation database   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: this function is extremely slow! use low top_n\n",
    "def get_gene_list(gwas_ht, top_n = 50):\n",
    "    gwas_ht_top = gwas_ht.filter(gwas_ht.gwas_rank < top_n) \n",
    "    mv = myvariant.MyVariantInfo()\n",
    "    gwas_df_top = gwas_ht_top.to_pandas()\n",
    "    gwas_df_top.rename(columns={\"locus.contig\": \"CHR\", \"locus.position\": \"BP\"}, inplace=True)\n",
    "    gwas_df_top[\"A1\"] = [gwas_df_top[\"alleles\"][i][0] for i in range(gwas_df_top.shape[0])]\n",
    "    gwas_df_top[\"A2\"] = [gwas_df_top[\"alleles\"][i][1] for i in range(gwas_df_top.shape[0])]\n",
    "    gwas_df_top.drop(\"alleles\", axis=1, inplace= True)\n",
    "    gwas_df_top[\"hgvs\"] = [gwas_df_top[\"CHR\"][i] + \":g.\" + str(gwas_df_top[\"BP\"][i]) + gwas_df_top[\"A1\"][i] + \">\" + gwas_df_top[\"A2\"][i] for i in range(gwas_df_top.shape[0])]\n",
    "    start = time.time()\n",
    "    l = []\n",
    "    # note: this is very slow, but the lookup with a list throws an internal server error\n",
    "    # mv.getvariants(gwas_df_top[\"hgvs\"], assembly=\"hg38\",fields=\"dbsnp.rsid\", as_dataframe=True) \"error ID required\"\n",
    "    for h in gwas_df_top['hgvs']:\n",
    "        #print(h)\n",
    "        q = mv.getvariant(h, assembly=\"hg38\", fields=\"dbsnp.gene\")\n",
    "        if q==None:\n",
    "            l.append(\"\")\n",
    "        else:\n",
    "            try:\n",
    "                l.append(q[\"dbsnp\"][\"gene\"][\"symbol\"])\n",
    "            except:\n",
    "                l.append(\"\")\n",
    "    gwas_df_top[\"gene\"] = l\n",
    "    end = time.time()\n",
    "    print(\"For \" + str(top_n) + \" variants annotation gene took \" + str(round(end-start)) + \" seconds\")\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipeline'></a>\n",
    "# Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if only_plink = True:\n",
    "    mt = plink_to_hl(plink_paths, hl_path)\n",
    "else:\n",
    "    mt = hl.read_matrix_table(hl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control\n",
    "Set parameters of QC functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant QC 1\n",
    "# generally recommended to filter variants before filtering samples\n",
    "print(\"variant qc I\")\n",
    "mt = hl.variant_qc(mt)\n",
    "mt = single_np(mt)\n",
    "mt = call_rate_filter_variants(mt, thresh=0.95)\n",
    "mt = maf_filter(mt, thresh=0.01)\n",
    "mt = ld_prune(mt, r2=0.2, window_size=500000) \n",
    "\n",
    "# Sample QC\n",
    "print(\"sample qc\")\n",
    "mt = hl.sample_qc(mt)\n",
    "mt = sex_discrepancy(mt)\n",
    "mt = call_rate_filter_samples(mt, thresh=0.95)\n",
    "mt = ibd_prune(mt, removal=\"max_ind_set\", set_min=0.125)\n",
    "mt = het_filter(mt)\n",
    "\n",
    "# Variant QC 2\n",
    "print(\"variant qc II\")\n",
    "mt = autosomes(mt) # filter to only autosomes after sex_discrepancy filter (uses gynosomes)\n",
    "mt =  hwe_filter(mt, case_control=\"control_sep\") # HWE filter after LD pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write out QCed matrix table\n",
    "mt.write(hl_path+name+\"_qc.mt\")\n",
    "# mt = hl.read_matrix_table(hl_path+name+\"_qc.mt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "Gives insight into population structure (ie ancestry)   \n",
    "__Note__: GWAS uses first 5 PCs, no option to alter this at a high level in this script yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PCA_visual:\n",
    "    # check out the first few principal components of PCA on this data\n",
    "    # this is not necessary, but useful to get an impression of the data\n",
    "    mt = choose_pcs(mt)\n",
    "else:\n",
    "    # only annotate PCs \n",
    "    # k must be >=5 for use with current GWAS logistic regression implementation\n",
    "    eigenvalues, scores, loadings = hl.hwe_normalized_pca(mt.GT, k=5)\n",
    "    mt = mt.annotate_cols(scores = scores[mt.s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mt with PCs\n",
    "mt.write(hl_path+name+\"_qc_pca.mt\")\n",
    "# mt = hl.read_matrix_table(hl_path+name+\"_qc_pca.mt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GWAS and VariantSpark\n",
    "Set parameters for VS here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GWAS: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_ht, mt = gwas_log_regression_1c(mt)\n",
    "# Set phenotype/name of dataset as title to produce manhattan plots with caption: \"Manhattan plot of \" +title+\", logistic regression\"\n",
    "qq_manh_plot(gwas_ht, title=\"Asthma\", sign_threshold=5e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variant Spark: random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_with_imp, mt = vs_rf_after_gwas(mt, gwas, n_trees=5000, mTry=0.1, seed=4957)\n",
    "# Set phenotype/name of dataset as title to produce manhattan plots with caption: \"Manhattan plot of \" +title+\", random forest\"\n",
    "vs_manhattan(gwas_with_imp, title=\"Asthma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hail table gwas_ht from GWAS and gwas_with_imp from VS\n",
    "gwas_ht.write(hl_path+\"results/\"+pheno+\"_gwas.ht\")\n",
    "gwas_with_imp_ht.write(hl_path+\"results/\"+pheno+\"_gwas_\"+str(ntrees)+\"trees.ht\")\n",
    "# save mt with annotations log_reg_pval, log_reg_log10p, log_reg_rank from GWAS and  vs_imp, vs_rank from VS\n",
    "# note that the mt now only contains variants with call_rate==1. This means log_reg_rank is probably incorrect\n",
    "mt.write(hl_path+name+\"_qc_pca_gwas_vs.mt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on x axis: -log10(logistic regression P-value)\n",
    "# on y axis: sqrt(VariantSpark Importance score)\n",
    "# title=\"Comparing GWAS with Variant Spark (\"+ title + \")\"\n",
    "comparison_scatter(gwas_with_imp, title=\"Asthma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benjamini Hochberg threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints variants that pass BH threshold \n",
    "# returns pandas table with all variants and their BH adjusted pvalues\n",
    "df = bh_thresh(mt, gwas_ht, fdr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for differences in cases and controls of top significant SNPs predicted by VariantSpark\n",
    "Plots MAF, gives stats about call rate and HWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 500\n",
    "top_vs_snps = gwas_with_imp.filter(gwas_with_imp.vs_rank < top_n, keep=True)\n",
    "noise_pred(mt, top_vs_snps, method=\"VS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GWAS catalog overlap\n",
    "Optional, please provide path to file from GWAS catalog in \"Parameters\"   \n",
    "(On [GWAS catalog](https://www.ebi.ac.uk/gwas/) search for phenotype (trait) and click Download Catalog data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns tables with top SNPs that overlap with a GWAs catalog SNP\n",
    "# size of window around GWAS catalog SNPs to overlap with\n",
    "delta = 50000\n",
    "if gwas_catalog_overlap:\n",
    "    gwas_catalog_to_intervals(gwas_catalog_filepath, delta = delta)\n",
    "    top_gwas_snps_region, top_vs_snps_region = check_for_overlaps_in_top_snps(gwas_with_imp, gwas_ht, top_n=500, ref_genome=ref_genome, interval_path=hl_path+\"gwas_catalog_intervals_\"+str(delta)+\"_windows.tsv\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polygenic Risk Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_rows(beta = gwas_ht[mt.locus, mt.alleles].beta)\n",
    "mt = mt.annotate_rows(importance = gwas_with_imp[mt.locus, mt.alleles].importance)\n",
    "mt = prs(mt, method=\"gwas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of PRS score with cases=1, controls=0\n",
    "# Histograms would make more sense, but cause hail timeout exception\n",
    "p = hl.plot.scatter(mt.is_case, mt.prs, xlabel='is_case', ylabel='PRS GWAS beta')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report prediction accuracy of PRS with test and training data\n",
    "# p=0.5 means 50/50 split, higher p means more samples in training set\n",
    "pred_accuracy_prs(mt, p=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest trained with the most significant SNPs from logistic regression and VS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_n = number of SNPs predicted to be most significant to use for training\n",
    "# n_trees is number of trees in sklearn tree\n",
    "prediction_accuracy(mt, top_n=200, p=0.5, n_trees=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genes of top SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns list of genes from myvariant.info annotation\n",
    "# WARNING: SLOW!\n",
    "l = get_gene_list(gwas_ht, top_n = 50)\n",
    "# unique genes\n",
    "list(set(l)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
